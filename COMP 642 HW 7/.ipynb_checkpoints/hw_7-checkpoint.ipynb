{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning - Clustering\n",
    "\n",
    "We've shown several unsupervised learning algorithms in this unit,  We first introduced the simplest clustering algorihtm k-means.  Hierarchical clustering included agglomerative and divisive clustering algorithms. Finally we discussed a powerful density-based algorihtm DBSCAN that performs fairly well when the clusters have irregular shapes, In this coding assignment you will explore more clustering algorithms and become familiar with the sklearn's clustering package. You'll also need to tune the hyper parameters of these models and get a sense about how the hyperparameters influence the shape of resulting clusters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "Load dataset with different cluster shapes and try algorithms on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_moons, make_circles, make_swiss_roll\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1500\n",
    "X_blobs, y_blobs = make_blobs(n_samples= n_samples, \n",
    "                  n_features=2, \n",
    "                  centers=3, \n",
    "                  cluster_std=0.5, \n",
    "                  shuffle=True, \n",
    "                  random_state=0)\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c='white', marker='o', s=50)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 170\n",
    "X_blobs1, y_blobs1 = make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X_blobs1, transformation)\n",
    "aniso = (X_aniso, y_blobs1)\n",
    "plt.scatter(aniso[0][:, 0], aniso[0][:, 1], c='white', marker='o', s=50)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_moons = make_moons(n_samples=n_samples, noise=.05)\n",
    "plt.scatter(noisy_moons[0][:, 0], noisy_moons[0][:, 1], c='white', marker='o', s=50)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_circles = make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "plt.scatter(noisy_circles[0][:, 0], noisy_circles[0][:, 1], c='white', marker='o', s=50)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varied = make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)\n",
    "plt.scatter(varied[0][:, 0], varied[0][:, 1], c='white', marker='o', s=50)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping objects by similarity using k-means\n",
    "\n",
    "K-means algorithm is simple but it performs poorly to enlongated clusters, or manifolds with irregular shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster(model, n_clusters, X):\n",
    "    y_km = model.fit_predict(X)\n",
    "    color_list = ['lightgreen', 'orange', 'lightblue', 'red', 'yellow', 'brown', 'cyan']\n",
    "    for i in range(n_clusters):\n",
    "        plt.scatter(X[y_km == i, 0],\n",
    "        X[y_km == i, 1],\n",
    "        s=50,\n",
    "        c=color_list[i],\n",
    "        marker='s',\n",
    "        label='cluster ' + str(i))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "km = KMeans(n_clusters=3, \n",
    "            init='random', \n",
    "            n_init=10, \n",
    "            max_iter=300,\n",
    "            tol=1e-04,\n",
    "            random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cluster(km, 3, X_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the elbow method to find the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distortions(data):\n",
    "    distortions = []\n",
    "    for i in range(1, 11):\n",
    "        km = KMeans(n_clusters=i, \n",
    "                init='k-means++', \n",
    "                n_init=10, \n",
    "                max_iter=300, \n",
    "                random_state=0)\n",
    "        km.fit(data)\n",
    "        distortions.append(km.inertia_)\n",
    "    plt.plot(range(1, 11), distortions, marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distortions(X_blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO :: run the kmeans on the other datasets and use elbow method to select the number of clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering\n",
    "\n",
    "Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering \n",
    "\n",
    "The algorithm performs a hierarchical clustering using a bottom up approach. Agglomerative cluster has a “rich get richer” behavior that leads to uneven cluster sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle, islice\n",
    "\n",
    "# Set up cluster parameters\n",
    "plt.figure(figsize=(9 * 1.3 + 2, 14.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {'n_neighbors': 10,\n",
    "                'n_clusters': 3}\n",
    "\n",
    "datasets = [\n",
    "    (noisy_circles, {'n_clusters': 2}),\n",
    "    (noisy_moons, {'n_clusters': 2}),\n",
    "    (varied, {'n_neighbors': 2}),\n",
    "    (aniso, {'n_neighbors': 2}),\n",
    "    ((X_blobs, y_blobs), {})]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ward = AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='ward')\n",
    "    complete = AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='complete')\n",
    "    average = AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='average')\n",
    "    \n",
    "    clustering_algorithms = (\n",
    "        ('Average Linkage', average),\n",
    "        ('Complete Linkage', complete),\n",
    "        ('Ward Linkage', ward),\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\n",
    "                \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as time\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "\n",
    "###############################################################################\n",
    "# Generate data (swiss roll dataset)\n",
    "n_samples = 1000\n",
    "noise = 0.05\n",
    "X, _ = make_swiss_roll(n_samples, noise)\n",
    "# Make it thinner\n",
    "X[:, 1] *= .5\n",
    "\n",
    "###############################################################################\n",
    "# Compute clustering\n",
    "print(\"Compute structured hierarchical clustering...\")\n",
    "st = time.time()\n",
    "ward = AgglomerativeClustering(\n",
    "        n_clusters=6, linkage='ward')\n",
    "ward.fit(X)\n",
    "if hasattr(algorithm, 'labels_'):\n",
    "    label = ward.labels_.astype(np.int)\n",
    "else:\n",
    "    label = ward.predict(X)\n",
    "print (\"Elapsed time: {}\".format(time.time() - st))\n",
    "\n",
    "###############################################################################\n",
    "# Plot result\n",
    "fig = pl.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.view_init(7, -80)\n",
    "for l in np.unique(label):\n",
    "    ax.plot3D(X[label == l, 0], X[label == l, 1], X[label == l, 2],\n",
    "              'o', color=pl.cm.jet(np.float(l) / np.max(label + 1)))\n",
    "pl.title('Without connectivity constraints')\n",
    "\n",
    "###############################################################################\n",
    "# Define the structure A of the data. Here a 10 nearest neighbors\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "connectivity = kneighbors_graph(X, n_neighbors=10)\n",
    "\n",
    "###############################################################################\n",
    "# Compute clustering\n",
    "print(\"Compute structured hierarchical clustering...\")\n",
    "st = time.time()\n",
    "ward = AgglomerativeClustering(\n",
    "        n_clusters=6, linkage='ward', connectivity=connectivity)\n",
    "ward.fit(X)\n",
    "if hasattr(algorithm, 'labels_'):\n",
    "    label = ward.labels_.astype(np.int)\n",
    "else:\n",
    "    label = ward.predict(X)\n",
    "print (\"Elapsed time: {}\".format(time.time() - st))\n",
    "\n",
    "###############################################################################\n",
    "# Plot result\n",
    "fig = pl.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.view_init(7, -80)\n",
    "for l in np.unique(label):\n",
    "    ax.plot3D(X[label == l, 0], X[label == l, 1], X[label == l, 2],\n",
    "              'o', color=pl.cm.jet(float(l) / np.max(label + 1)))\n",
    "pl.title('With connectivity constraints')\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Based on the code above, what is the difference between the two models? Which one performs better and why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN algorithm\n",
    "\n",
    "As opposed to k-means algorithm which assumes that clusters are convex shape, DBSCAN views clusters as areas of high density separated by areas of low density. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.3, min_samples=10).fit(X_blobs)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y_blobs, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(y_blobs, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(y_blobs, labels))\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X_blobs[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = X_blobs[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.3, min_samples=10).fit(noisy_moons[0])\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = noisy_moons[0][class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = noisy_moons[0][class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.3, min_samples=10).fit(noisy_circles[0])\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = noisy_circles[0][class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = noisy_circles[0][class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "eps and min_samples are two important parameters for the DBSCAN model.  What are those two parameters? Tune the parameters of the model for noisy_circles and noisy_moon dataset to make it separate the clusters perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean-Shift Algorithm\n",
    "\n",
    "Similiar to K-means, Mean shift algorithm locates the centroids of the clusters by shifting the points to density function maxima. Here is the procedure of the algorithm.\n",
    "\n",
    "\n",
    "1. For each datapoint x ∈ X, find the neighbouring points N(x) of x.\n",
    "\n",
    "2. For each datapoint x ∈ X, calculate the mean shift m(x).\n",
    "\n",
    "3. For each datapoint x ∈ X, update x ← m(x).\n",
    "\n",
    "4. Repeat 1. for n_iterations or until the points are almost not moving or not moving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def euclid_distance(x, xi):\n",
    "    return np.sqrt(np.sum((x - xi)**2))\n",
    "\n",
    "def neighbourhood_points(X, x_centroid, distance = 5):\n",
    "    eligible_X = []\n",
    "    for x in X:\n",
    "        distance_between = euclid_distance(x, x_centroid)\n",
    "        # print('Evaluating: [%s vs %s] yield dist=%.2f' % (x, x_centroid, distance_between))\n",
    "        if distance_between <= distance:\n",
    "            eligible_X.append(x)\n",
    "    return eligible_X\n",
    "\n",
    "def gaussian_kernel(distance, bandwidth):\n",
    "    val = (1/(bandwidth*math.sqrt(2*math.pi))) * np.exp(-0.5*((distance / bandwidth))**2)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 150\n",
    "X_blobs, y_blobs = make_blobs(n_samples= n_samples, \n",
    "                  n_features=2, \n",
    "                  centers=3, \n",
    "                  cluster_std=0.5, \n",
    "                  shuffle=True, \n",
    "                  random_state=0)\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c='white', marker='o', s=50)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = X_blobs\n",
    "# freeze the original points and make a copy\n",
    "copy_points = np.copy(original_data)\n",
    "\n",
    "look_distance = 1  # How far to look for neighbours.\n",
    "kernel_bandwidth = 8  # Kernel parameter.\n",
    "# a list to save the history move to do the visualization\n",
    "past_X = []\n",
    "n_iteration = 3\n",
    "for i in range(n_iteration):\n",
    "    \n",
    "    for index, x in enumerate(copy_points):\n",
    "        \n",
    "        # for each datapoint x, find the neighbouring points N(x) of x\n",
    "        neighbors = neighbourhood_points(copy_points, x, look_distance)\n",
    "        # calculate the mean shift m(x)\n",
    "         \n",
    "        denominator = 0\n",
    "        nominator = 0\n",
    "        for n in neighbors:\n",
    "            weight = gaussian_kernel(euclid_distance(x, n), kernel_bandwidth)\n",
    "            nominator += weight * n\n",
    "            denominator += weight\n",
    "        new_x = nominator/(denominator+1e-4)\n",
    "        copy_points[index] = new_x\n",
    "    past_X.append(np.copy(copy_points))\n",
    "         \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(1)\n",
    "figure.set_size_inches((7, 20))\n",
    "plt.subplot(n_iteration+2, 1, 1)\n",
    "plt.title('Initial state')\n",
    "plt.plot(original_data[:,0], original_data[:,1], 'bo')\n",
    "plt.plot(original_data[:,0], original_data[:,1], 'ro')\n",
    "\n",
    "for i in range(n_iteration):\n",
    "    figure_index = i + 2\n",
    "    plt.subplot(n_iteration+2, 1, figure_index)\n",
    "    plt.title('Iteration: %d' % (figure_index - 1))\n",
    "    plt.plot(original_data[:,0], original_data[:,1], 'bo')\n",
    "    plt.plot(past_X[i][:,0], past_X[i][:,1], 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "look_distance and kernel_bandwidth are two important parameters for mean shift algorithm. kernel_bandwidth simply determines the size of neighborhood over which the density will be computed. look_distance determines the region to look at when searching the surrounding neighbors. Tune these two parameters to make the cluster centroids exactly 3 for the above dataset. What is proper look_distance and kernel_bandwidth in your setting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Apply any of these algorithms to your favorite dataset.  Possible applications of clustering algorihtm will include but not be limited to image segmentation and outlier detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
